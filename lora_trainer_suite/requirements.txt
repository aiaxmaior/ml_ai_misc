# LoRA Trainer Suite Requirements
# Optimized for RTX 3090, 128GB RAM, Ryzen 9 7900X
# Updated: January 2025

# Core Dependencies (CUDA 12.8 compatible)
torch>=2.9.0
torchvision>=0.21.0
torchaudio>=2.9.0

# Diffusion Models
diffusers>=0.35.0
transformers>=4.57.0
accelerate>=1.3.0
safetensors>=0.5.0
peft>=0.15.0

# Quantization & Optimization
bitsandbytes>=0.45.0
flash-attn>=2.7.0
xformers>=0.0.30
triton>=3.2.0

# vLLM for high-performance inference (optional but recommended)
vllm>=0.8.0

# CLIP Interrogator
clip-interrogator>=0.6.0
open_clip_torch>=2.29.0

# Vision-Language Models (Qwen2-VL / Qwen2.5-VL / Qwen3-VL)
# Note: Qwen2.5-VL and Qwen3-VL use the same packages as Qwen2-VL
qwen-vl-utils>=0.0.8
einops>=0.8.0
# For video support (optional): av>=13.0.0

# Image Processing
Pillow>=11.0.0
opencv-python>=4.11.0
albumentations>=1.4.0

# GUI
gradio>=5.15.0
plotly>=5.24.0

# Data Processing
numpy>=2.2.0
pandas>=2.2.0
tqdm>=4.67.0

# Configuration
pyyaml>=6.0.2

# Training Frameworks (optional - install separately)
# kohya-ss - install from https://github.com/kohya-ss/sd-scripts
# ai-toolkit - install from https://github.com/ostris/ai-toolkit

# Utilities
rich>=13.9.0
colorama>=0.4.6
python-dotenv>=1.0.1

# Development (optional)
pytest>=8.3.0
black>=24.10.0
flake8>=7.1.0

# Optional: WandB for experiment tracking
# wandb>=0.19.0

# Optional: Tensorboard
# tensorboard>=2.18.0
# tensorboardX>=2.6.2

# ==============================================================================
# Additional Installation Notes
# ==============================================================================

# CUDA Installation:
# - These packages are compatible with CUDA 12.8
# - Install PyTorch with CUDA 12.8:
#   pip install torch==2.9.1+cu128 torchvision==0.21.1+cu128 torchaudio==2.9.1+cu128 \
#     --index-url https://download.pytorch.org/whl/cu128
# - For other CUDA versions, adjust the cu128 suffix accordingly

# Qwen Vision Models (Qwen2-VL / Qwen2.5-VL / Qwen3-VL):
# - Latest models work with transformers>=4.57.0
# - For abliterated (uncensored) versions, check community repos on HuggingFace
# - Examples: mlabonne/Qwen2-VL-7B-Instruct-abliterated (if available)
# - Qwen3-VL requires transformers>=4.57.0

# vLLM Setup (for 3-5x faster inference):
# 1. Install: pip install vllm>=0.8.0
# 2. Start server:
#    vllm serve Qwen/Qwen2.5-VL-7B-Instruct \
#      --max-model-len 4096 \
#      --dtype bfloat16 \
#      --gpu-memory-utilization 0.8 \
#      --trust-remote-code
# 3. Server runs on http://localhost:8000 by default
# 4. GUI will auto-detect and use vLLM if available
# 5. vLLM 0.8.0+ includes improved vision model support

# For best results with RTX 3090:
# - Use vLLM for batch tagging operations (3-5x faster)
# - Use direct mode if vLLM server is not running
# - 4-bit quantization works only in direct mode (not with vLLM)
# - Flash Attention 2.7.0+ provides better memory efficiency
# - Gradio 5.x includes improved video/GIF preview support
